{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: abaglione and lihuacai\n",
    "\n",
    "Credit to Tyler Spears and Sonia Baee, who developed the precursor\n",
    "to this preprocessing script\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import pathlib\n",
    "import glob\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "import random\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pipeline\n",
    "\n",
    "from sklearn import impute\n",
    "from sklearn import datasets\n",
    "from sklearn import svm, linear_model, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "plt.rcParams.update({'figure.facecolor': [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# configure autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the weekly feature vectors \n",
    "all_feats = pd.read_csv('features/app_users_only/all_ind_wkly.csv')\n",
    "all_feats.drop(axis=1,columns=['Unnamed: 0'] + \\\n",
    "               [col for col in all_feats.columns if 'trait' in col and 'group' in col],\n",
    "               inplace=True)\n",
    "all_feats['pid'] = all_feats['pid'].astype(str)\n",
    "\n",
    "# Store the feature names\n",
    "featnames = list(all_feats.columns)\n",
    "all_feats.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Classification Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcomes transformation -- anx, dep\n",
    "all_feats['anx'].hist()\n",
    "all_feats['dep'].hist()\n",
    "\n",
    "#add classification outcomes\n",
    "''' Note to self: made these both adhere to the same threshold, since they had the same scale\n",
    "    Not sure why depression was >=4 but anxiety was >=3, originally.'''\n",
    "\n",
    "all_feats['dep_cat'] = np.where(all_feats['dep'] >= 4, 1, 0)\n",
    "all_feats['anx_cat'] = np.where(all_feats['anx'] >= 4, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COL = 'PID'\n",
    "TIMEDIV_COL = 'week_of_study'\n",
    "OUTCOME_COLS = ['anx', 'dep', 'anx_cat', 'dep_cat']\n",
    "APPS = ['aspire', 'boostme', 'dailyfeats', 'icope', 'mantra', 'messages',\n",
    "        'moveme', 'relax', 'slumbertime', 'thoughtchallenger', 'worryknot']\n",
    "ENGAGEMENT_METRICS = ['frequency', 'duration',\n",
    "                      'betweenlaunch_duration', 'days_of_use']\n",
    "TIMES_OF_DAY = ['morning', 'afternoon', 'evening', 'late_night']\n",
    "\n",
    "# Survey Features Only\n",
    "survey_fs_cols = ['cope_alcohol_tob', 'physical_pain', 'connected', 'receive_support', 'active',\n",
    "                  'support_others', 'healthy_food']\n",
    "\n",
    "# Get a list of columns indicating which app(s) were used the most often\n",
    "mua_dummies = [col for col in all_feats.columns if 'most_used_app' in col]\n",
    "\n",
    "# App Features - Aggregate, Across All Apps\n",
    "app_overall_fs_cols = ['frequency', 'daysofuse', 'duration', 'duration_mean',\n",
    "                       'duration_std', 'duration_min', 'duration_max', 'betweenlaunch_duration_mean',\n",
    "                       'betweenlaunch_duration_std', 'num_apps_used']\n",
    "\n",
    "# App Features - From Individual Apps\n",
    "app_ind_fs_cols = [col for col in all_feats.columns\n",
    "     if any([app in col for app in APPS])\n",
    "     and any([agg_metric in col for agg_metric in ENGAGEMENT_METRICS])\n",
    "     and not any([tod in col for tod in TIMES_OF_DAY])\n",
    "     ]\n",
    "\n",
    "app_ind_fs_cols[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset with survey features + app features from only the most used apps\n",
    "mua_dfs = []\n",
    "mua_dummies = [col for col in all_feats.columns if 'most_used_app' in col]\n",
    "\n",
    "# First, drop the aggregate engagement feature columns\n",
    "survey_app_mua_feats = all_feats.drop(app_overall_fs_cols)    \n",
    "\n",
    "# Now, iterate through the dataframe. For each observation (row):\n",
    "for i in range(all_feats.shape[0]):\n",
    "\n",
    "    # Get the current row as a dataframe\n",
    "    df = survey_app_mua_feats.iloc[[i]]\n",
    "\n",
    "    # Find the most used apps - retain only the first one\n",
    "    df2 = df[mua_dummies]\n",
    "    most_used_apps = list(df2.columns[(df2 == 1).any(axis=0)])\n",
    "    most_used_app = [col.replace('most_used_app_', '') for col in most_used_apps][0]\n",
    "    \n",
    "    # Eliminate individual app columns that aren't for the most used app\n",
    "    df2 = df.drop([col for col in survey_app_mua_feats.columns \n",
    "                  if most_used_app not in col and\n",
    "                  any([app in col for app in APPS])\n",
    "                  ])\n",
    "\n",
    "    # Finally, remove the name of the most used app from any column names\n",
    "    df2.columns = df2.columns.str.replace('_' + most_used_app, '')\n",
    "    mua_dfs.append(df2)\n",
    "\n",
    "# Replace the temp dataframe with a concat of all the individual row dfs\n",
    "# This is our final df\n",
    "survey_app_mua_feats = pd.concat(mua_dfs, sort=False)\n",
    "survey_app_mua_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add most_used_app dummy columns to app-related featuresets\n",
    "app_overall_fs_cols += mua_dummies\n",
    "app_ind_fs_cols += mua_dummies\n",
    "\n",
    "'''Define one last set of feature columns:\n",
    "\n",
    "    App Features: Features from Most Used Apps Only \n",
    "\n",
    "    Same column names as app_overall_fs_cols, \n",
    "    but we'll use a different dataframe depending on the name of the featureset.\n",
    "    This will become more clear in the prediction tasks below!\n",
    "    '''\n",
    "\n",
    "app_mua_fs_cols = app_overall_fs_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of featuresets\n",
    "featuresets = {\n",
    "    'survey_fs': survey_fs_cols,\n",
    "    'app_overall_fs': app_overall_fs_cols,\n",
    "    'app_ind_fs': app_ind_fs_cols,\n",
    "    'app_mua_fs': app_mua_fs_cols,\n",
    "    'survey_app_overall_fs': survey_fs_cols+app_overall_fs_cols, \n",
    "    'survey_app_ind_fs': survey_fs_cols+app_ind_fs_cols,\n",
    "    'survey_app_mua_fs': survey_fs_cols+app_mua_fs_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of all NaNs\n",
    "all_feats.dropna(axis=1, how='all', inplace=True)\n",
    "survey_app_mua_feats.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######regression tasks on 1-5 scale (cut off on both 1 (floor) and 5 (ceiling)) using lasso linear mixed effect model;\n",
    "alpha_list = np.arange(0.1, 0.81, 0.1)\n",
    "lmm_res = []\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    print('alpha: {0}'.format(alpha))\n",
    "    for fs_name, fs_cols in featuresets.items():\n",
    "        if 'mua' in fs_name:\n",
    "            df = survey_app_mua_feats.copy()\n",
    "        else:\n",
    "            df = all_feats.copy()\n",
    "        \n",
    "        df['intercept'] = 1\n",
    "        df.to_csv('features/%s.csv' % fs_name)\n",
    "        \n",
    "        for target in ['anx', 'dep']:\n",
    "            res = pipeline.genMixedLM(df, target, ['intercept'] + fs_cols,\n",
    "                            'pid', fs_name, alpha=alpha)\n",
    "            lmm_res.append(res.copy())\n",
    "\n",
    "lmm_res = pd.concat(lmm_res, copy=True, ignore_index=True, sort=False)\n",
    "lmm_res.to_csv('results/lmm_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "id_col = 'pid'\n",
    "target_cols = ['anx_cat', 'dep_cat']\n",
    "\n",
    "for fs_name, fs_cols in featuresets.items():\n",
    "\n",
    "    if 'mua' not in fs_name:\n",
    "        df = all_feats\n",
    "    else:\n",
    "        # Handle special cases in which we want data only from the most used app\n",
    "        df = survey_app_mua_feats\n",
    "\n",
    "    X = df[[id_col] + fs_cols].copy()\n",
    "    \n",
    "    ''' If this is a featureset with app features \n",
    "        Get a list of one-hot-encoded columns from the most_used_app feature.'''\n",
    "    mua_onehots = [col for col in X.columns if 'most_used_app' in col]\n",
    "    \n",
    "    print(X.columns)\n",
    "    # Get categorical feature indices - will be used with SMOTENC later\n",
    "    nominal_idx = sorted([X.columns.get_loc(c) for c in ['pid'] + mua_onehots])\n",
    "\n",
    "    # y\n",
    "    targets = {\n",
    "        'anxiety': df['anx_cat'].copy(),\n",
    "        'depression': df['dep_cat'].copy()\n",
    "    }\n",
    "\n",
    "    for target_name, target_col in targets.items():\n",
    "        for method in ['RF', 'XGB']:\n",
    "            res = pipeline.classifyMood(X=X, y=target_col, id=id_col, target=target_name,\n",
    "                              nominal_idx = nominal_idx, fs=fs_name, method=method)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc57a2896280f2c2a3102ad67cd3c73f9b6766cabe916e2ab016cfe9bc8c475a"
  },
  "kernelspec": {
   "display_name": "int-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
