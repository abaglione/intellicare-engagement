{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: abaglione and lihuacai\n",
    "\n",
    "Credit to Tyler Spears and Sonia Baee, who developed the precursor\n",
    "to this preprocessing script\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import pathlib\n",
    "import glob\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "import random\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pipeline\n",
    "\n",
    "from sklearn import impute\n",
    "from sklearn import datasets\n",
    "from sklearn import svm, linear_model, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "plt.rcParams.update({'figure.facecolor': [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# configure autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the weekly feature vectors \n",
    "all_feats = pd.read_csv('features/' + pipeline.APP_USERS_DIR + 'all_ind_wkly.csv')\n",
    "all_feats.drop(axis=1,columns=['Unnamed: 0'] + \\\n",
    "               [col for col in all_feats.columns if 'trait' in col and 'group' in col],\n",
    "               inplace=True)\n",
    "all_feats['pid'] = all_feats['pid'].astype(str)\n",
    "\n",
    "# Store the feature names\n",
    "featnames = list(all_feats.columns)\n",
    "all_feats.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Classification Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Note to self: made these both adhere to the same threshold, since they had the same scale\n",
    "    Not sure why depression was >=4 but anxiety was >=3, originally.'''\n",
    "\n",
    "all_feats['dep_cat'] = np.where(all_feats['dep'] >= 4, 1, 0)\n",
    "all_feats['anx_cat'] = np.where(all_feats['anx'] >= 4, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats['dep_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats['anx_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COL = 'PID'\n",
    "TIMEDIV_COL = 'week_of_study'\n",
    "OUTCOME_COLS = ['anx', 'dep', 'anx_cat', 'dep_cat']\n",
    "APPS = ['aspire', 'boostme', 'dailyfeats', 'icope', 'mantra', 'messages',\n",
    "        'moveme', 'relax', 'slumbertime', 'socialforce', 'thoughtchallenger', 'worryknot']\n",
    "ENGAGEMENT_METRIC_PREFIXES = ['frequency', 'daysofuse', 'duration', 'betweenlaunch']\n",
    "TIMES_OF_DAY = ['morning', 'afternoon', 'evening', 'late_night']\n",
    "\n",
    "# Survey Features Only\n",
    "survey_fs_cols = ['cope_alcohol_tob', 'physical_pain', 'connected', 'receive_support', 'active',\n",
    "                  'support_others', 'healthy_food']\n",
    "\n",
    "# Get a list of columns indicating which app(s) were used the most often\n",
    "mua_dummies = [col for col in all_feats.columns if 'most_used_app' in col]\n",
    "\n",
    "# App Features - Aggregate, Across All Apps\n",
    "app_overall_fs_cols = ['frequency', 'daysofuse', 'duration', 'duration_mean',\n",
    "                       'duration_std', 'duration_min', 'duration_max', 'betweenlaunch_duration_mean',\n",
    "                       'betweenlaunch_duration_std']\n",
    "\n",
    "# App Features - From Individual Apps\n",
    "app_ind_fs_cols = [col for col in all_feats.columns\n",
    "     if any([app in col for app in APPS])\n",
    "     and any([prefix in col for prefix in ENGAGEMENT_METRIC_PREFIXES])\n",
    "     and not any([tod in col for tod in TIMES_OF_DAY])\n",
    "     ]\n",
    "\n",
    "app_ind_fs_cols[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset with survey features + app features from only the most used apps\n",
    "mua_dfs = []\n",
    "MUA_PREFIX = 'most_used_app_' # One-hot encoded/dummitized columns indicating most used app(s)\n",
    "mua_dummies = [col for col in all_feats.columns if MUA_PREFIX in col]\n",
    "\n",
    "# First, drop the aggregate engagement feature columns\n",
    "survey_app_mua_feats = all_feats.drop(columns=app_overall_fs_cols)    \n",
    "\n",
    "# Now, iterate through the dataframe. For each observation (row):\n",
    "for i in range(all_feats.shape[0]):\n",
    "\n",
    "    # Get the current row as a dataframe\n",
    "    df = survey_app_mua_feats.iloc[[i]]\n",
    "\n",
    "    # Find the most used apps - retain only the first one\n",
    "    df2 = df[mua_dummies]\n",
    "    most_used_app_cols = list(df2.columns[(df2 == 1).any(axis=0)])\n",
    "    most_used_apps = [col.replace(MUA_PREFIX, '') for col in most_used_app_cols]\n",
    "    \n",
    "    for mua in most_used_apps:\n",
    "        \n",
    "        '''Eliminate individual app columns that aren't for the most used app\n",
    "           However, retain the \"most_used_app\" dummitized columns! ''' \n",
    "        df2 = df.drop(columns = [col for col in survey_app_mua_feats.columns \n",
    "                                 if mua not in col\n",
    "                                 and MUA_PREFIX not in col\n",
    "                                 and any([app in col for app in APPS])])\n",
    "\n",
    "        ''' Remove the name of the most used app from all columns EXCEPT\n",
    "            the dummitized \"most_used_app\" columns. This enables a clean pd.concat later on.'''\n",
    "        df2.rename(mapper=lambda x: x.replace('_' + mua, '') if MUA_PREFIX not in x else x,\n",
    "                   axis=1, inplace=True)\n",
    "\n",
    "        ''' Finally, set all other dummitized \"most_used_app\" columns to 0, since we are \n",
    "            creating separate dfs for each \"most used app\"\n",
    "        '''\n",
    "        mua_dummies_subset = list(set(mua_dummies) - set([MUA_PREFIX + mua]))\n",
    "        df2[mua_dummies_subset] = 0\n",
    "        mua_dfs.append(df2)\n",
    "\n",
    "# Replace the temp dataframe with a concat of all the individual row dfs\n",
    "# This is our final df\n",
    "survey_app_mua_feats = pd.concat(mua_dfs, sort=False)\n",
    "survey_app_mua_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add most_used_app dummy columns to app-related featuresets\n",
    "app_overall_fs_cols += mua_dummies\n",
    "app_ind_fs_cols += mua_dummies\n",
    "\n",
    "# Other mods / additions\n",
    "app_mua_fs_cols = app_overall_fs_cols.copy()\n",
    "app_overall_fs_cols += ['num_apps_used']\n",
    "app_overall_fs_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mua_fs_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of featuresets\n",
    "featuresets = {\n",
    "    'survey_fs': survey_fs_cols,\n",
    "    'app_overall_fs': app_overall_fs_cols,\n",
    "    'app_ind_fs': app_ind_fs_cols,\n",
    "    'app_mua_fs': app_mua_fs_cols,\n",
    "    'survey_app_overall_fs': survey_fs_cols+app_overall_fs_cols, \n",
    "    'survey_app_ind_fs': survey_fs_cols+app_ind_fs_cols,\n",
    "    'survey_app_mua_fs': survey_fs_cols+app_mua_fs_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of all NaNs\n",
    "all_feats.dropna(axis=1, how='all', inplace=True)\n",
    "survey_app_mua_feats.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_app_mua_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######regression tasks on 1-5 scale (cut off on both 1 (floor) and 5 (ceiling)) using lasso linear mixed effect model;\n",
    "# TODO - change so not passing in whole df every time\n",
    "alpha_list = np.arange(0.1, 0.81, 0.1)\n",
    "lmm_res = []\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    print('alpha: {0}'.format(alpha))\n",
    "    for fs_name, fs_cols in featuresets.items():\n",
    "        if 'mua' in fs_name:\n",
    "            df = survey_app_mua_feats\n",
    "        else:\n",
    "            df = all_feats\n",
    "        \n",
    "        df['intercept'] = 1\n",
    "        df.to_csv('features/%s.csv' % fs_name)\n",
    "        \n",
    "        for target in ['anx', 'dep']:\n",
    "            res = pipeline.genMixedLM(df, target, ['intercept'] + fs_cols,\n",
    "                            'pid', fs_name, alpha=alpha)\n",
    "            lmm_res.append(res.copy())\n",
    "\n",
    "lmm_res = pd.concat(lmm_res, copy=True, ignore_index=True, sort=False)\n",
    "lmm_res.to_csv('results/lmm_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "id_col = 'pid'\n",
    "targets = {\n",
    "    'depression': 'dep_cat'\n",
    "}\n",
    "\n",
    "for fs_name, fs_cols in featuresets.items():\n",
    "    print(fs_name)\n",
    "    if 'app' in fs_name:\n",
    "        if 'mua' not in fs_name:\n",
    "            df = all_feats\n",
    "        else:\n",
    "            # Handle special cases in which we want data only from the most used app\n",
    "            df = survey_app_mua_feats\n",
    "\n",
    "        for target_name, target_col in targets.items():  \n",
    "            # Drop rows where target is NaN - should never impute these!\n",
    "            df2 = df.dropna(subset=[target_col], how='any')\n",
    "        \n",
    "            X = df2[[id_col] + fs_cols]\n",
    "            y = df2[target_col]\n",
    "\n",
    "            ''' If this is a featureset with app features \n",
    "                Get a list of one-hot-encoded columns from the most_used_app feature.'''\n",
    "            mua_onehots = [col for col in X.columns if MUA_PREFIX in col]\n",
    "\n",
    "            # Get categorical feature indices - will be used with SMOTENC later\n",
    "            nominal_idx = sorted([X.columns.get_loc(c) for c in ['pid'] + mua_onehots])\n",
    "\n",
    "            for method in ['RF', 'XGB']:\n",
    "                res = pipeline.classifyMood(X=X, y=y, id_col=id_col, target=target_name,\n",
    "                                            nominal_idx = nominal_idx, fs=fs_name, method=method)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc57a2896280f2c2a3102ad67cd3c73f9b6766cabe916e2ab016cfe9bc8c475a"
  },
  "kernelspec": {
   "display_name": "Python [conda env:int-eng] *",
   "language": "python",
   "name": "conda-env-int-eng-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
