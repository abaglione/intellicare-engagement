{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: abaglione and lihuacai\n",
    "\n",
    "Credit to Tyler Spears and Sonia Baee, who developed the precursor\n",
    "to this preprocessing script\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import pathlib\n",
    "import glob\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "import random\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pipeline\n",
    "\n",
    "from sklearn import impute\n",
    "from sklearn import datasets\n",
    "from sklearn import svm, linear_model, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "plt.rcParams.update({'figure.facecolor': [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# configure autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data file paths and associated names\n",
    "# Set up data directories\n",
    "datafiles = pipeline.gather_files(pipeline.DATA_CLEANED_DIR)\n",
    "\n",
    "# Import data into dataframes and standardize column names and values\n",
    "items = list(datafiles.items())\n",
    "master_dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize the data\n",
    "for f, fname in items:\n",
    "    df = pd.read_csv(f, parse_dates=True)\n",
    "\n",
    "    # Record which file (dataset) this dataframe came from\n",
    "    df.insert(0, 'dataset', fname)\n",
    "    \n",
    "    # Make colnames lowercase (thank you, Chris Albon!)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "\n",
    "    # Remove irrelevant PID cols - special case\n",
    "    part_id_names = ['eim_id', 'pid_code', 'id']\n",
    "    possible_irrelevant = 'pid'\n",
    "    \n",
    "    part_id_cols = [col for col in df.columns if col in part_id_names]\n",
    "        \n",
    "    if possible_irrelevant in df.columns and len(part_id_cols) > 0:\n",
    "        df.drop(possible_irrelevant, axis=1, inplace=True)\n",
    "    \n",
    "    # Standardize participant ids\n",
    "    standard_part_id_name = 'pid'\n",
    "    \n",
    "    if len(part_id_cols) > 0:\n",
    "        id_change = [\n",
    "            (i if col in part_id_names else -1) for i, col in enumerate(df.columns)\n",
    "        ]\n",
    "\n",
    "        id_change = df.columns[sorted(id_change)[-1]]\n",
    "        df.rename(index=str, columns={id_change: standard_part_id_name}, inplace=True)\n",
    "\n",
    "    # Remove 'EIM' prefix from pid\n",
    "    df['pid'] = df['pid'].replace(\"^(EIM)\", \"\", regex=True)\n",
    "    df['pid'] = pd.to_numeric(df['pid'], errors='coerce')\n",
    "    df = df.dropna(subset=['pid'])\n",
    "    df['pid'] = df['pid'].astype(int)\n",
    "    \n",
    "    # Correction of PHQ4_change col in pre_post scores sheet\n",
    "    if fname == 'pre_post':\n",
    "        df['phq4_change'] = df['phq_post'] - df['phq_bl']\n",
    "    \n",
    "    # Fix PHQ-4 column for baseline survey\n",
    "    if fname == 'blsurvey':\n",
    "        df.drop('phq4', axis=1, inplace=True)\n",
    "        df.rename(columns={'sc0': 'phq4'}, inplace=True)\n",
    "\n",
    "    if fname == 'app_launch':            \n",
    "        # Standardize app names\n",
    "        df['package'] = df['package'].replace('^(edu.northwestern.cbits.intellicare.)', '', regex=True)\n",
    "        \n",
    "        # Filter out push notifications\n",
    "        df = df[df.package != 'conductor']\n",
    "        \n",
    "#         # Convert duration to minutes\n",
    "#         df['duration'] = df['duration'] / 60.0\n",
    "#         df['duration'] = df['duration'].round(1)\n",
    "\n",
    "        # Remove outliers\n",
    "        df = df[np.abs(scipy.stats.zscore(df['duration'])) < 3]\n",
    "        \n",
    "    # Standardize date columns\n",
    "    date_keywords = ['timestamp', 'date']\n",
    "    date_cols = [col for col in df.columns if any(keyword in col for keyword in date_keywords)]\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        \n",
    "    if fname != 'fwsurveys':\n",
    "        to_drop = [col for col in date_cols if 'timestamp' in col and 'new' not in col]  \n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    # Drop columns related to enrollment date that Lee generated, for weekly surveys\n",
    "    if fname == 'wklysurvey':\n",
    "        to_drop = [col for col in df.columns if 'enroll' in col]  \n",
    "        df.drop(to_drop, axis=1, inplace=True) \n",
    "                \n",
    "    if fname == 'wklysurvey_timestamps':\n",
    "        df.rename(index=str, columns={'sent (0/1)': 'sent',\n",
    "                                     'completed? (0/1)': 'completed'}, inplace=True)\n",
    "        \n",
    "    if fname == 'fwsurveys' or fname == 'wklysurvey':\n",
    "        # Rename columns for easier reading\n",
    "        df.rename(index=str, columns=pipeline.CODEBOOK_MAPPINGS, inplace=True)\n",
    "        \n",
    "        # Convert the depression mood to its reverse scale to be consistent with anxiety mood\n",
    "        # The higher the anxiety and depression mood scores are, the worse they are (e.g., more anxious or depressed)\n",
    "        df['state_dep'] = df['state_dep'].map({1:5,2:4,3:3,4:2,5:1})\n",
    "\n",
    "    # Save out to CSVs \n",
    "    # We'll call these the 'processed' files, since the ones we read in were already somewhat clean\n",
    "    fpath = pathlib.Path.joinpath(pipeline.DATA_PROCESSED_DIR, fname + '_processed.csv')\n",
    "    df.to_csv(fpath)\n",
    "    \n",
    "    # Group dataframes into one list \n",
    "    master_dataset[fname] = df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast_cancer",
   "language": "python",
   "name": "breast_cancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
